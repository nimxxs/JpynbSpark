{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23648ee6-51dd-47a1-ba4e-147287a0af11",
   "metadata": {},
   "source": [
    "# spark 기본개념 잡기 : RDD\n",
    "+ 여러 분산노드에 걸쳐 저장하는 변경불가 데이터 집합을 의미\n",
    "+ RDD 생성은 직접 만들거나 파일을 통해 생성할 수 있음\n",
    "+ RDD는 transformation과 action으로 구성\n",
    "   - 기존 RDD의 데이터를 토대로 새로운 RDD를 만들어 냄\n",
    "   - RDD를 기반으로 무언가를 계산해서 결과를 만들어 냄\n",
    "+ RDD는 `Lazy` 로딩 방식을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705894f-a269-4675-af17-ab96cce77754",
   "metadata": {},
   "source": [
    "## 스파크 중요 개념 : RDD, dataframe\n",
    "* RDD : 탄력적이고 분산된 데이터셋\n",
    "* HDFS와는 달리 쓰기 불가능 데이터셋\n",
    "* 다양한 연산(map, reduce, count, filter, join) 수행 가능\n",
    "* 작업은 lazy하게 병렬로 수행되고 메모리에 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe165a-a4f5-4301-bba7-225fb3ba07a9",
   "metadata": {},
   "source": [
    "## History of Spark API\n",
    "* RDD   (2011)\n",
    "    + v1 부터 지원, 분산 데이터셋\n",
    "    + 연산을 제어하는 코드 작성이 어려움\n",
    "* dataframe (2013)\n",
    "    + v1.3부터 지원\n",
    "    + 데이터를 스키마형태로 추상화, 속도 개선\n",
    "* dataset (2015)\n",
    "    + v1.6부터 지원\n",
    "    + 데이터의 자료형 검사, 직렬화 지원\n",
    "* dataset (2016)\n",
    "    + v2.0부터 지원\n",
    "    + dataframe과 dataset을 dataset으로 통합\n",
    "* 스파크 애플리케이션 개발 : RDD 이용, SparkContext 사용\n",
    "* SQL on Spark : dataset,dataframe 이용, SparkSession 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7106d8-f024-43de-b161-ee6c122d82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# README.md 파일 읽어들임\n",
    "lines = sc.textFile('/usr/share/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda5a642-77ce-4ab3-9146-8d60eb42abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Structured Streaming for stream processing.',\n",
       " '',\n",
       " '<https://spark.apache.org/>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인들 중 10줄만 확인 : collect\n",
    "lines.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f908c8e-dff3-4187-88b7-cf62b8f35474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인수 확인 : count\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4abb9d7-1900-49ad-9868-f60adb1f74dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " '[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " '## Building Spark',\n",
       " 'Spark is built using [Apache Maven](https://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 읽어들인 라인들 중 Spark라는 단어를 찾음 : filter(transformation)\n",
    "filterLines = lines.filter(lambda x: \"Spark\" in x)\n",
    "filterLines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a10f0-d09e-4c6e-ae8a-5d2e76745e27",
   "metadata": {},
   "source": [
    "### Lazy 로딩방식 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e6cd73-d994-4d82-a601-7c69d0f057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못된 위치의 파일을 읽으려고 시도 - 오류 출력이 안됨\n",
    "lines = sc.textFile('usr/share/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea6eb6-ffd9-41b0-939b-ea577bf24c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy loading에 의해서 collect라는 action이 호출이 되어야만 비로소 오류 출력이 가능함!\n",
    "lines.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52c795-3049-4b51-87f4-dbf4ffc33786",
   "metadata": {},
   "source": [
    "## RDD 생성\n",
    "+ 직접 생성한 데이터로 만들거나\n",
    "   - sc.parallelize(리스트)\n",
    "+ 외부 데이터로 만드는 방법 존재\n",
    "   - sc.textFile(경로/파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38136a12-da14-4d1c-a835-12c5d5f27e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Hello, World!!','Hello, Python!!','Hello, RDD!!']\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97149e-a946-4900-bc88-b3ce63fba742",
   "metadata": {},
   "source": [
    "### 직책별 사원수 조회하는 RDD 코드 작성\n",
    "+ employees.csv 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3558594-f2e6-4416-8576-7e3656988f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID',\n",
       " '100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000.00,,,90',\n",
       " '101,Neena,Kochhar,NKOCHHAR,515.123.4568,2005-09-21,AD_VP,17000.00,,100,90',\n",
       " '102,Lex,De Haan,LDEHAAN,515.123.4569,2001-01-13,AD_VP,17000.00,,100,90',\n",
       " '103,Alexander,Hunold,AHUNOLD,590.423.4567,2006-01-03,IT_PROG,9000.00,,102,60']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = sc.textFile('data/employees.csv')\n",
    "emp.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49ae075-aab8-4f73-b553-55535a5ca5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤더 제외하고 데이터만 골라냄\n",
    "header = emp.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b940acb1-43f2-4ed3-9160-d8b7bdfbf423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100,Steven,King,SKING,515.123.4567,2003-06-17,AD_PRES,24000.00,,,90'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = emp.filter(lambda x: header != x)\n",
    "emp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74980dd9-854b-42bf-be16-b3d0997d1c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 'Steven'),\n",
       " ('AD_VP', 'Neena'),\n",
       " ('AD_VP', 'Lex'),\n",
       " ('IT_PROG', 'Alexander'),\n",
       " ('IT_PROG', 'Bruce')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사원 데이터에서 ,로 각 컬럼을 분리하고\n",
    "# 이름과 직책을 추출함\n",
    "emp2 = emp.map(lambda x: (x.split(',')[6], x.split(',')[1]))  # key 와 value를 넣어야함\n",
    "emp2.collect()[:5]  # collect를 꼭 써야만 진정한 실행이 되는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610ba454-e978-439d-8848-8057d13cf7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1), ('AD_VP', 1), ('AD_VP', 1), ('IT_PROG', 1), ('IT_PROG', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출한 데이터에서 직책을 1로 매핑\n",
    "maps = emp2.mapValues(lambda x: 1)\n",
    "maps.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a320be-b072-436a-bb9f-12e1405b19cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AD_PRES', 1),\n",
       " ('AD_VP', 2),\n",
       " ('IT_PROG', 5),\n",
       " ('FI_MGR', 1),\n",
       " ('FI_ACCOUNT', 5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 직책끼리 모아서 집계처리함\n",
    "reduces = maps.reduceByKey(lambda x, y: x + y)\n",
    "reduces.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a08817-830e-43b4-8c06-de7be8746ceb",
   "metadata": {},
   "source": [
    "### 타이타닉 승객의 생존자/사망자 수를 조회하는 RDD 코드를 작성하세요\n",
    "+ titanic.csv를 이용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46c488ea-6bd0-47f6-9b55-ffa6f203489d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pclass,survived,name,sex,age,sibsp,parch,ticket,fare,cabin,embarked',\n",
       " '1,1,\"Allen, Miss. Elisabeth Walton\",female,29,0,0,24160,211.3375,B5,S',\n",
       " '1,1,\"Allison, Master. Hudson Trevor\",male,0.9167,1,2,113781,151.5500,C22 C26,S',\n",
       " '1,0,\"Allison, Miss. Helen Loraine\",female,2,1,2,113781,151.5500,C22 C26,S',\n",
       " '1,0,\"Allison, Mr. Hudson Joshua Creighton\",male,30,1,2,113781,151.5500,C22 C26,S']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = sc.textFile('data/titanic.csv')\n",
    "titanic.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "62b56bda-f0e7-4dfb-a74f-c06b6549e64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,1,\"Allen, Miss. Elisabeth Walton\",female,29,0,0,24160,211.3375,B5,S'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 헤더 제외하고 데이터만 골라냄\n",
    "header = titanic.first()\n",
    "titanic = titanic.filter(lambda x: header != x)\n",
    "titanic.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7b3800e-8557-4771-81b8-6b2c252f7688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'female'),\n",
       " ('1', 'male'),\n",
       " ('0', 'female'),\n",
       " ('0', 'male'),\n",
       " ('0', 'female')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타이타닉 데이터에서 ,로 각 컬럼을 분리하고\n",
    "# 생존자/사망자 수를 추출함\n",
    "titanic2 = titanic.map(lambda x: (x.split(',')[1], x.split(',')[4]))  # key 와 value를 넣어야함\n",
    "titanic2.collect()[:5]  # collect를 꼭 써야만 진정한 실행이 되는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c86af7ca-1326-4350-a26a-6d4bd4506636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1), ('1', 1), ('0', 1), ('0', 1), ('0', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = titanic2.mapValues(lambda x: 1)\n",
    "maps.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6f589f7-9ace-471d-904a-e66e4b77392d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 500), ('0', 809), ('', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduces = maps.reduceByKey(lambda x, y: x + y)\n",
    "reduces.collect()[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
